# -*- coding: utf-8 -*-
"""fingerprint-based-bloodgroup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kaY0tSzfd_po_b2LagbWBpMsbDFNEk_W
"""

import numpy as np
import pandas as pd

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

IMG_HEIGHT = 64
IMG_WIDTH = 128
BATCH_SIZE = 32

# Directory paths
train_dir = '/content/dataset_blood_group/'

# Data augmentation and preprocessing for training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2  # Use 20% of the data for validation
)

# Train generator
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

# Validation generator
validation_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

# Low accuracy model with Input layer
def create_low_accuracy_model():
    model = tf.keras.models.Sequential([
        tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),
        tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')
    ])

    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Create the low accuracy model
low_acc_model = create_low_accuracy_model()

# Train the low accuracy model
history_low_acc = low_acc_model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=10,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    validation_steps=validation_generator.samples // BATCH_SIZE
)

low_acc_model.save('low_accuracy_model.h5')
low_acc_model.save('low_accuracy_model.keras')

# Evaluate the low accuracy model
low_acc_eval = low_acc_model.evaluate(validation_generator)
print(f"Low Accuracy Model - Loss: {low_acc_eval[0]}, Accuracy: {low_acc_eval[1]}")

# Image dimensions
IMG_HEIGHT = 64
IMG_WIDTH = 64
BATCH_SIZE = 32

# Directory paths (adjust as needed)
train_dir = '/content/dataset_blood_group/'

# Data augmentation and preprocessing for training
train_datagen = ImageDataGenerator(
    rescale=1./255,  # Normalize pixel values to [0,1]
    validation_split=0.2  # Use 20% of the data for validation
)

# Train generator
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'  # Training data subset
)

# Validation generator
validation_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'  # Validation data subset
)

# High accuracy model: Deep CNN
def create_high_accuracy_model():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dropout(0.5),  # Regularization to prevent overfitting
        tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')
    ])

    # Compile the model
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Create the model
high_acc_model = create_high_accuracy_model()

# Train the model
history_high_acc = high_acc_model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=100,  # Adjust the number of epochs based on your preference
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    validation_steps=validation_generator.samples // BATCH_SIZE
)

high_acc_model.save('high_accuracy_model.h5')
high_acc_model.save('high_accuracy_model.keras')


# Evaluate the model on validation data
high_acc_eval = high_acc_model.evaluate(validation_generator)
print(f"High Accuracy Model - Loss: {high_acc_eval[0]}, Accuracy: {high_acc_eval[1]}")

import tensorflow as tf
import numpy as np
from tensorflow.keras.utils import load_img, img_to_array

# Load the trained model
model = tf.keras.models.load_model('high_accuracy_model.h5')

# Path to the test image
test_image_path = 'cluster_5_950.BMP'

# Image dimensions (should match the dimensions used during training)
IMG_HEIGHT = 64
IMG_WIDTH = 64

# Preprocess the image
image = load_img(test_image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))  # Load and resize the image
image_array = img_to_array(image)  # Convert the image to a NumPy array
image_array = image_array / 255.0  # Normalize the pixel values to [0, 1]
image_array = np.expand_dims(image_array, axis=0)  # Add the batch dimension

# Predict the class
predictions = model.predict(image_array)

# Get the index of the predicted class
predicted_class_index = np.argmax(predictions, axis=1)

# Get the class labels from the model's train generator (use this if you trained using flow_from_directory)
class_labels = {v: k for k, v in train_generator.class_indices.items()}

# Map the predicted class index to the class label
predicted_label = class_labels[predicted_class_index[0]]

# Print the predicted class label
print(f"Predicted Class: {predicted_label}")